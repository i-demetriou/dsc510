{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01cf6162-98e9-4142-9676-f8b6b4c1cd33",
   "metadata": {},
   "source": [
    "## Getting The \"Optimal\" Number Of Clusters: K-Means and Hierarchical Clustering\n",
    "\n",
    "Please go through this file carefully and REPLACE all instances of the word None with the appropriate commands.\n",
    "\n",
    "** Submit the finalized .ipynb file to Moodle by Thursday, December 2, 23:59. **\n",
    "\n",
    "## [A] K-Means\n",
    "\n",
    "K-means clustering is an unsupervised algorithm. In an unsupervised algorithm, we are not interested in making predictions (since we don’t have a target/output variable). The objective is to discover interesting patterns in the data, e.g., are there any subgroups or 'cluster' in the data.\n",
    "\n",
    "In this assignment, discuss the most important parameter of K-Means i.e., the ways by which we can select an optimal number of clusters (K)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca6ff3-8cb3-4db1-afa3-9f554f8f546c",
   "metadata": {},
   "source": [
    "#### Create an artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0b1032-d9d9-4e16-b9cb-c8e4db8f2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.data import wine_data\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "df = pd.DataFrame({'X1': [3, 1, 1, 2, 1, 6, 6, 6, 5, 6, 7, 8, 9, 8, 9, 9, 8],\n",
    "                  'X2': [5, 4, 5, 6, 5, 8, 6, 7, 6, 7, 1, 2, 1, 2, 3, 2, 3]})\n",
    "\n",
    "# create the scatter plot of the two features (X1 on the x-axis and X2 on the y-axis)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96b88f-9259-46bb-8d0f-ef54da0a07b0",
   "metadata": {},
   "source": [
    "Visually we can see that the optimal number of clusters should be around 3. But visualizing the data alone cannot always give the right answer.\n",
    "\n",
    "### 1. The Elbow Method\n",
    "The elbow method runs K-means clustering on the dataset for a range of values of K, say 1 to 10.\n",
    "\n",
    "- Perform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points.\n",
    "- Plot these points and find the point where the average distance from the centroid falls suddenly (“Elbow”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43739065-70c4-4870-a522-316c895d0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(X):\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(1,10)\n",
    "    for num_clusters in K :\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(X)\n",
    "        Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "    plt.plot(K,Sum_of_squared_distances,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Sum of squared distances/Inertia') \n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "# get the values of the dataframe\n",
    "X = None\n",
    "# call the elbow_method with X as input\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4085ec3-9810-42a9-a164-7f8b2cbdbdfc",
   "metadata": {},
   "source": [
    "The curve looks like an elbow. In the above plot, the elbow is at K=3, i.e. the sum of squared distances falls suddenly, indicating the optimal K for this dataset is 3.\n",
    "\n",
    "### 2. Silhouette analysis\n",
    "The silhouette coefficient is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "- Select a range of values of k (say 1 to 10).\n",
    "- Plot Silhouette coefﬁcient for each value of K.\n",
    "\n",
    "The equation for calculating the silhouette coefﬁcient for a particular data point:\n",
    "\n",
    "$$S(i) = \\frac{b(i) - a(i)}{max\\{a(i), b(i)\\}}$$\n",
    "\n",
    "- S(i) is the silhouette coefficient of the data point i.\n",
    "- a(i) is the average distance between i and all the other data points in the cluster to which i belongs.\n",
    "- b(i) is the average distance from i to all clusters to which i does not belong.\n",
    "\n",
    "We will then calculate the average_silhouette for every K.\n",
    "\n",
    "$$AverageSilhouette = mean\\{S(i)\\}$$\n",
    "\n",
    "Then plot the graph between average_silhouette and K.\n",
    "\n",
    "Points to remember while calculating silhouette coefficient:\n",
    "\n",
    "- The value of the silhouette coefﬁcient is between [-1, 1].\n",
    "- A score of 1 denotes the best meaning that the data point i is very compact within the cluster to which it belongs and far away from the other clusters.\n",
    "- The worst value is -1. Values near 0 denote overlapping clusters.\n",
    "\n",
    "Let us see the python code with help of an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2bb7c8-8829-4878-9780-f31291fcad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(X):\n",
    "    range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "    silhouette_avg = []\n",
    "    for num_clusters in range_n_clusters:\n",
    "        # initialise kmeans\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(X)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        # silhouette score\n",
    "        silhouette_avg.append(silhouette_score(X, cluster_labels))\n",
    "\n",
    "    plt.plot(range_n_clusters,silhouette_avg,'bx-')\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Silhouette score') \n",
    "    plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()\n",
    "    \n",
    "# run silhouette method to create the figure shown below\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d666b1-9de6-417f-a864-73bc9f18d615",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see that the silhouette score is maximized at k = 3. So, we will take 3 clusters.\n",
    "\n",
    "NOTE: The silhouette Method is used in combination with the Elbow Method for a more confident decision.\n",
    "\n",
    "### [B] Hierarchical Clustering\n",
    "\n",
    "In K-means clustering, the number of clusters that you want to divide your data points into i.e., the value of K has to be pre-determined whereas in Hierarchical clustering data is automatically formed into a tree shape form (dendrogram).\n",
    "\n",
    "So how do we decide which clustering to select? We choose either of them depending on our problem statement and business requirement.\n",
    "\n",
    "Hierarchical clustering gives you a deep insight into each step of converging different clusters and creates a dendrogram. It helps you to figure out which cluster combination makes more sense.\n",
    "\n",
    "There are mainly two types of hierarchical clustering:\n",
    "\n",
    "- Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root). The result is a tree which can be plotted as a dendrogram.\n",
    "- Divisive hierarchical clustering: It’s also known as DIANA (Divise Analysis) and it works in a top-down manner. The algorithm is an inverse order of AGNES. It begins with the root, in which all objects are included in a single cluster. At each step of iteration, the most heterogeneous cluster is divided into two. The process is iterated until all objects are in their own cluster.\n",
    "\n",
    "Note that agglomerative clustering is good at identifying small clusters. Divisive hierarchical clustering is good at identifying large clusters.\n",
    "\n",
    "#### Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad9a12c-1f01-4485-b8c2-492d52eb71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "def hierarchical_clustering(X):\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    anglomer_model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "    anglomer_model = anglomer_model.fit(X)\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    # plot the dendrogram\n",
    "    plot_dendrogram(anglomer_model)\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.ylabel(\"Distance (or dissimilarity) between clusters\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# run hierarchical_clustering function\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b673240-3306-45ca-a950-b88d1432d3ff",
   "metadata": {},
   "source": [
    "##### Working with Dendrograms\n",
    "In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\n",
    "\n",
    "The height of the fusion (vertical line), provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\n",
    "\n",
    "The height of the cut (horizontal line --> threshold) to the dendrogram controls the number of clusters obtained. There is no definitive answer on how to set this threshold, since cluster analysis is essentially an exploratory approach; the interpretation of the resulting hierarchical structure is context-dependent and often several solutions are equally good from a theoretical point of view.\n",
    "\n",
    "In the figure above, it seems that there exist 3 clusters of points at the bottom of the dendrogram. However, the elbow method and the silhouette analysis seem more robust ways of determining the best K value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b6fe2-09b8-4af6-b65c-9ccaf6b355ff",
   "metadata": {},
   "source": [
    "### Cluster wines of the wine dataset\n",
    "\n",
    "The wine dataset is a classic and very easy multi-class classification dataset. It contains 178 observations of 13 features which are categorized into three classes (cultivars). Here, we will use only the features of the 178 wines in order to find the best number of clusters that these wines can be separated into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075e2c46-126d-4803-9120-f5c960710093",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = wine_data()\n",
    "\n",
    "# use the KNN classification algorithm with default parameters\n",
    "knn = None\n",
    "# run sequential forward selection (SFS) method to extract the smallest number of features (trying from 3 to 13 features) that maximizes the accuracy\n",
    "# use parameter forward as True, floating as False, verbose = 0, accuracy as the socring method and 10-fold cross validation\n",
    "sfs = None\n",
    "# train the SFS method\n",
    "sfs = None\n",
    "# When you create the SFS model, uncomment the following 2 lines\n",
    "#plot_sfs(sfs.get_metric_dict(), kind='std_err');\n",
    "#print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255de23c-3309-414e-8664-8015e9f5a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the proposed number of (important) features \n",
    "X_selected = pd.DataFrame()\n",
    "X_selected[\"alcohol\"] = X[:,0]\n",
    "X_selected[\"ash\"] = X[:,6]\n",
    "X_selected[\"proanthocyanins\"] = X[:,9]\n",
    "# print statistical information of the dataframe\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1ccad-ac29-4cc3-95af-151b525d5bc9",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "\n",
    "From the statistical properties of the selected features, we argue that they spread in relatively different range. Inevitably, scaling seems to be an important prep-processing step.\n",
    "\n",
    "According to [1], it can be concluded that standardization before clustering algorithm leads to obtain a better quality, efficient and accurate cluster result. It is also important to select a\n",
    "specific standardization procedure, according to the nature of the datasets for the analysis. In this analysis [1], the Z-score (standardization of features by removing the mean and scaling to unit variance) was proposed as the most powerful method that will give more accurate and efficient results. Below, we also use the Z-score (StandardScaler) to scale the feature set.\n",
    "\n",
    "[1] I. Mohamad, D. Usman, \"Standardization and Its Effects on K-Means Clustering Algorithm\", Research Journal of Applied Sciences, Engineering and Technology 6(17): 3299-3303, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883224fa-3800-4110-955d-ea975b888024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the standard scaler with default parameters to scale the selected features\n",
    "X_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9a47ac-4d8e-4859-9a94-b969264c1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the elbow method with the scaled dataframe\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1e5285-29a8-4717-bc27-a8d71b85b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the silhouette analysis with the scaled dataframe\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf36b8e5-c968-4c3d-b326-c4f517c565fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the hierarchical clustering function with the scaled dataframe\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc4064-d5ab-425d-ab73-5c2b450d4287",
   "metadata": {},
   "source": [
    "### Cluster Visualization \n",
    "\n",
    "Based on the the number of clusters evaluated from the above analysis, we use K-Means to cluster the 178 wine observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9dd778e-137b-4e2b-852b-d266fc00b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KMeans algorithm and set the n_clusters parameter to be equal to the best K obtained from the analysis above\n",
    "kmeans_model = None\n",
    "# fit scaled dataset to separate the 178 observations into the selected number of clusters \n",
    "kmeans_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee74d972-21e2-46c8-aaad-5ab47f59a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe containing the scaled data as well as the labels assigned to each wine\n",
    "clusters = pd.DataFrame(X_scaled, columns = [\"alcohol\", \"ash\", \"proanthocyanins\"])\n",
    "# when you create the k-Means model, uncomment the following line\n",
    "#clusters['label']=kmeans_model.labels_\n",
    "# print the first 5 lines of the dataframe\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48700918-8f21-4164-b800-8edf3da67069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig = px.scatter_3d(clusters, x=\"alcohol\", y=\"ash\", z=\"proanthocyanins\", color=\\'label\\')\\n\\nfig.update_layout(\\n    title=\\'Wine clusters\\',\\n    width=600, height=600,\\n    margin=dict(t=30, r=0, l=20, b=10)\\n)\\n\\ncamera = dict(\\n    eye=dict(x=-1.25, y=1.25, z=1.75)\\n)\\n\\nfig.update_layout(scene_camera=camera)\\nfig.show()\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print each wine as a point in the 3D space\n",
    "# when you prepare the clusters dataframe, uncomment the following lines\n",
    "\"\"\"\n",
    "fig = px.scatter_3d(clusters, x=\"alcohol\", y=\"ash\", z=\"proanthocyanins\", color='label')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Wine clusters',\n",
    "    width=600, height=600,\n",
    "    margin=dict(t=30, r=0, l=20, b=10)\n",
    ")\n",
    "\n",
    "camera = dict(\n",
    "    eye=dict(x=-1.25, y=1.25, z=1.75)\n",
    ")\n",
    "\n",
    "fig.update_layout(scene_camera=camera)\n",
    "fig.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1069235-b5b6-45c8-bd90-4db2419d18e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
