{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a24490-50de-447d-a78b-7cc123548c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0026e76-9ed5-4786-ac21-b4bc7f7fa813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                          body_text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read tab separated values\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', names=['label', 'body_text'], header=None)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d324b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12576\\761587215.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text).get_text()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_notags</th>\n",
       "      <th>text_nopunct</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah i dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         text_notags  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                      Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  U dun say so early hor... U c already then say...   \n",
       "4  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_nopunct  \\\n",
       "0  Go until jurong point crazy Available only in ...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                         text_nostop  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4      nah i dont think goes usf lives around though  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # punctuation characters in string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "punctuation = string.punctuation\n",
    "stop_words = stopwords.words('english')\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove html tags: gets a line of text as input and returns it without html tags.\n",
    "def remove_htmltags(text):\n",
    "    return BeautifulSoup(text).get_text()\n",
    "\n",
    "# function takes a line of text as input and discards all punctuation\n",
    "# returns a new line without punctuation\n",
    "def remove_punct(text):    \n",
    "    text = \"\".join([char for char in text if char not in punctuation])\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    text = \" \".join([word.lower() for word in tokens if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "data['text_notags'] = data['body_text'].apply(remove_htmltags)\n",
    "data['text_nopunct'] = data['text_notags'].apply(remove_punct)\n",
    "data['text_nostop'] = data['text_nopunct'].apply(remove_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4745b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_notags</th>\n",
       "      <th>text_nopunct</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>nah i dont think goes usf lives around though</td>\n",
       "      <td>nah i dont think goe usf live around though</td>\n",
       "      <td>nah i dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                         text_notags  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                      Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  U dun say so early hor... U c already then say...   \n",
       "4  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_nopunct  \\\n",
       "0  Go until jurong point crazy Available only in ...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                         text_nostop  \\\n",
       "0  go jurong point crazy available bugis n great ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3                u dun say early hor u c already say   \n",
       "4      nah i dont think goes usf lives around though   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  go jurong point crazi avail bugi n great world...   \n",
       "1                              ok lar joke wif u oni   \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21...   \n",
       "3                u dun say earli hor u c alreadi say   \n",
       "4        nah i dont think goe usf live around though   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4         nah i dont think go usf life around though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "wl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def stemming(text):    \n",
    "    tokens = word_tokenize(text) \n",
    "    stemmed_text = \" \".join([ps.stem(word) for word in tokens])\n",
    "    return stemmed_text\n",
    "\n",
    "def lemmatizing(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = \" \".join([wl.lemmatize(word) for word in tokens])\n",
    "    return lemmatized_text\n",
    "\n",
    "data['text_stemmed'] = data['text_nostop'].apply(stemming)\n",
    "data['text_lemmatized'] = data['text_nostop'].apply(lemmatizing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e2644e-956c-47b1-b243-6d610a9f04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('the', 'DT'), ('Introduction', 'NNP'), ('to', 'TO'), ('Data', 'NNP'), ('Science', 'NNP'), ('and', 'CC'), ('Analytics', 'NNP'), ('course', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on the Introduction to Data Science and Analytics course\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "850743c1-1444-4b2c-93be-4dd1c6af39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 'be'), ('be', 'or'), ('or', 'not'), ('not', 'to'), ('to', 'be')]\n"
     ]
    }
   ],
   "source": [
    "text = \"To be or not to be\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "bigrm = nltk.ngrams(tokens,2)\n",
    "print(list(bigrm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b24bc8-b320-4601-b761-13148d5e1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'good' 'is' 'long' 'movie' 'not' 'scary' 'slow' 'spooky' 'this'\n",
      " 'very']\n",
      "[[1 0 1 1 1 0 1 0 0 1 1]\n",
      " [1 0 2 0 1 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This movie is very scary and long', 'This movie is not scary and is slow', 'This movie is spooky and good']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0fa94a8-f729-4b3a-9bb7-4fea84196e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good' 'long' 'movi' 'scari' 'slow' 'spooki']\n",
      "[[0 1 1 1 0 0]\n",
      " [0 0 1 1 1 0]\n",
      " [1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "# Function to remove punctuation, turn to lowercase, remove stopwords, stem / lemmatize and tokenize\n",
    "def clean_text(text):\n",
    "    # remove punctuation:\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        \n",
    "    # turn to lowercase:\n",
    "    text = \" \".join([word.lower() for word in text.split()])\n",
    "        \n",
    "    # remove stop words:\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "   \n",
    "    # perform stemming:\n",
    "    text = \" \".join([ps.stem(word) for word in text.split()])\n",
    "    \n",
    "    # tokenize and return\n",
    "    return word_tokenize(text)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=clean_text)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df07c63-c692-4a87-ba6d-eb48be83f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n",
      "['and' 'good' 'is' 'long' 'movie' 'not' 'scary' 'slow' 'spooky' 'this'\n",
      " 'very']\n",
      "[[0.29628336 0.         0.29628336 0.50165133 0.29628336 0.\n",
      "  0.38151877 0.         0.         0.29628336 0.50165133]\n",
      " [0.26359985 0.         0.5271997  0.         0.26359985 0.44631334\n",
      "  0.3394328  0.44631334 0.         0.26359985 0.        ]\n",
      " [0.32052772 0.54270061 0.32052772 0.         0.32052772 0.\n",
      "  0.         0.         0.54270061 0.32052772 0.        ]]\n",
      "        idf_weights\n",
      "and        1.000000\n",
      "good       1.693147\n",
      "is         1.000000\n",
      "long       1.693147\n",
      "movie      1.000000\n",
      "not        1.693147\n",
      "scary      1.287682\n",
      "slow       1.693147\n",
      "spooky     1.693147\n",
      "this       1.000000\n",
      "very       1.693147\n",
      "           tfidf\n",
      "and     0.296283\n",
      "good    0.000000\n",
      "is      0.296283\n",
      "long    0.501651\n",
      "movie   0.296283\n",
      "not     0.000000\n",
      "scary   0.381519\n",
      "slow    0.000000\n",
      "spooky  0.000000\n",
      "this    0.296283\n",
      "very    0.501651\n",
      "           tfidf\n",
      "and     0.263600\n",
      "good    0.000000\n",
      "is      0.527200\n",
      "long    0.000000\n",
      "movie   0.263600\n",
      "not     0.446313\n",
      "scary   0.339433\n",
      "slow    0.446313\n",
      "spooky  0.000000\n",
      "this    0.263600\n",
      "very    0.000000\n",
      "           tfidf\n",
      "and     0.320528\n",
      "good    0.542701\n",
      "is      0.320528\n",
      "long    0.000000\n",
      "movie   0.320528\n",
      "not     0.000000\n",
      "scary   0.000000\n",
      "slow    0.000000\n",
      "spooky  0.542701\n",
      "this    0.320528\n",
      "very    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.shape)\n",
    "# print the vocabulary\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# print tf-idf matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names_out(),columns=[\"idf_weights\"]) \n",
    "# sort ascending \n",
    "print(df_idf)\n",
    "\n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tfidf_matrix[0] \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tfidf\"]) \n",
    "print(df)\n",
    "#get tfidf vector for second document \n",
    "second_document_vector=tfidf_matrix[1] \n",
    "#print the scores \n",
    "df2 = pd.DataFrame(second_document_vector.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tfidf\"]) \n",
    "print(df2)\n",
    "#get tfidf vector for third document \n",
    "third_document_vector=tfidf_matrix[2] \n",
    "#print the scores \n",
    "df3 = pd.DataFrame(third_document_vector.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tfidf\"]) \n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64ac431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.52000123 0.37986812]\n",
      " [0.52000123 1.         0.4224553 ]\n",
      " [0.37986812 0.4224553  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
